[2024-01-05T16:48:32.980+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-01T00:00:00+00:00 [queued]>
[2024-01-05T16:48:32.995+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-01T00:00:00+00:00 [queued]>
[2024-01-05T16:48:32.999+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 1
[2024-01-05T16:48:33.204+0000] {taskinstance.py:2192} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_data> on 2024-01-01 00:00:00+00:00
[2024-01-05T16:48:33.211+0000] {standard_task_runner.py:60} INFO - Started process 12637 to run task
[2024-01-05T16:48:33.223+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_data', 'scheduled__2024-01-01T00:00:00+00:00', '--job-id', '49', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp1dny6q9f']
[2024-01-05T16:48:33.224+0000] {standard_task_runner.py:88} INFO - Job 49: Subtask transform_twitter_data
[2024-01-05T16:48:33.339+0000] {task_command.py:423} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-01T00:00:00+00:00 [running]> on host airflow.vm.local
[2024-01-05T16:48:33.424+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_data' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T00:00:00+00:00'
[2024-01-05T16:48:33.435+0000] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-05T16:48:33.446+0000] {spark_submit.py:366} INFO - Spark-Submit cmd: spark-submit --master local --name Twitter Transformation --queue root.default /home/eder/Desktop/Airflow/Twitter/src/spark/transformation.py --src /home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-01 --dest /home/eder/Desktop/Airflow/Twitter/data/processed/ --process_date 2024-01-01
[2024-01-05T16:48:37.505+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:37 WARN Utils: Your hostname, airflow resolves to a loopback address: 127.0.1.1; using 192.168.1.79 instead (on interface enp0s3)
[2024-01-05T16:48:37.508+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-01-05T16:48:38.846+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-05T16:48:40.626+0000] {spark_submit.py:536} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-01-05T16:48:40.697+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:40 INFO SparkContext: Running Spark version 3.1.3
[2024-01-05T16:48:40.904+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:40 INFO ResourceUtils: ==============================================================
[2024-01-05T16:48:40.905+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:40 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-05T16:48:40.906+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:40 INFO ResourceUtils: ==============================================================
[2024-01-05T16:48:40.908+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:40 INFO SparkContext: Submitted application: Data Transformation
[2024-01-05T16:48:41.016+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-05T16:48:41.048+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:41 INFO ResourceProfile: Limiting resource is cpu
[2024-01-05T16:48:41.050+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:41 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-05T16:48:41.304+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:41 INFO SecurityManager: Changing view acls to: eder
[2024-01-05T16:48:41.307+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:41 INFO SecurityManager: Changing modify acls to: eder
[2024-01-05T16:48:41.307+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:41 INFO SecurityManager: Changing view acls groups to:
[2024-01-05T16:48:41.308+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:41 INFO SecurityManager: Changing modify acls groups to:
[2024-01-05T16:48:41.318+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(eder); groups with view permissions: Set(); users  with modify permissions: Set(eder); groups with modify permissions: Set()
[2024-01-05T16:48:42.343+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:42 INFO Utils: Successfully started service 'sparkDriver' on port 40593.
[2024-01-05T16:48:42.591+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:42 INFO SparkEnv: Registering MapOutputTracker
[2024-01-05T16:48:42.786+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:42 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-05T16:48:42.891+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-05T16:48:42.897+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-05T16:48:42.927+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-05T16:48:42.999+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ee5edd9d-e72f-44c9-87d4-30cfca640115
[2024-01-05T16:48:43.103+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-01-05T16:48:43.531+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:43 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-05T16:48:44.886+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-05T16:48:45.246+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://airflow.Home:4040
[2024-01-05T16:48:46.134+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:46 INFO Executor: Starting executor ID driver on host airflow.Home
[2024-01-05T16:48:46.308+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39821.
[2024-01-05T16:48:46.309+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:46 INFO NettyBlockTransferService: Server created on airflow.Home:39821
[2024-01-05T16:48:46.313+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-05T16:48:46.359+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, airflow.Home, 39821, None)
[2024-01-05T16:48:46.513+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:46 INFO BlockManagerMasterEndpoint: Registering block manager airflow.Home:39821 with 366.3 MiB RAM, BlockManagerId(driver, airflow.Home, 39821, None)
[2024-01-05T16:48:46.565+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, airflow.Home, 39821, None)
[2024-01-05T16:48:46.566+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, airflow.Home, 39821, None)
[2024-01-05T16:48:48.855+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:48 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/eder/Desktop/Airflow/Twitter/spark-warehouse').
[2024-01-05T16:48:48.863+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:48 INFO SharedState: Warehouse path is 'file:/home/eder/Desktop/Airflow/Twitter/spark-warehouse'.
[2024-01-05T16:48:52.718+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:52 INFO InMemoryFileIndex: It took 205 ms to list leaf files for 1 paths.
[2024-01-05T16:48:53.043+0000] {spark_submit.py:536} INFO - 24/01/05 16:48:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-01-05T16:49:02.288+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:02 INFO FileSourceStrategy: Pushed Filters:
[2024-01-05T16:49:02.291+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:02 INFO FileSourceStrategy: Post-Scan Filters:
[2024-01-05T16:49:02.300+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-01-05T16:49:03.295+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-01-05T16:49:03.540+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 366.0 MiB)
[2024-01-05T16:49:03.555+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on airflow.Home:39821 (size: 27.4 KiB, free: 366.3 MiB)
[2024-01-05T16:49:03.587+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:03 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T16:49:03.635+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212389 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T16:49:04.483+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:04 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T16:49:04.583+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:04 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T16:49:04.586+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:04 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T16:49:04.616+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:04 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T16:49:04.624+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:04 INFO DAGScheduler: Missing parents: List()
[2024-01-05T16:49:04.672+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T16:49:05.299+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 366.0 MiB)
[2024-01-05T16:49:05.326+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-01-05T16:49:05.333+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on airflow.Home:39821 (size: 6.3 KiB, free: 366.3 MiB)
[2024-01-05T16:49:05.346+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-01-05T16:49:05.427+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T16:49:05.439+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-01-05T16:49:05.720+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
[2024-01-05T16:49:05.844+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-01-05T16:49:06.951+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:06 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-01/%202024-01-01.json, range: 0-18085, partition values: [empty row]
[2024-01-05T16:49:08.016+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:08 INFO CodeGenerator: Code generated in 555.70456 ms
[2024-01-05T16:49:08.740+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-01-05T16:49:08.836+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3133 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T16:49:08.872+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-01-05T16:49:08.900+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:08 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 4.101 s
[2024-01-05T16:49:08.935+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T16:49:08.944+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-01-05T16:49:08.989+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:08 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 4.472841 s
[2024-01-05T16:49:10.799+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:10 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-01-05T16:49:10.803+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:10 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-01-05T16:49:10.806+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:10 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-01-05T16:49:11.283+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T16:49:11.284+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T16:49:11.287+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T16:49:11.759+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:11 INFO CodeGenerator: Code generated in 82.431788 ms
[2024-01-05T16:49:11.853+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:11 INFO CodeGenerator: Code generated in 51.606681 ms
[2024-01-05T16:49:11.863+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-01-05T16:49:11.878+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-01-05T16:49:11.891+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on airflow.Home:39821 (size: 27.5 KiB, free: 366.2 MiB)
[2024-01-05T16:49:11.896+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:11 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T16:49:11.914+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212389 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T16:49:12.120+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T16:49:12.123+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T16:49:12.123+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T16:49:12.123+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T16:49:12.123+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO DAGScheduler: Missing parents: List()
[2024-01-05T16:49:12.168+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T16:49:12.541+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 189.7 KiB, free 365.5 MiB)
[2024-01-05T16:49:12.556+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.4 KiB, free 365.4 MiB)
[2024-01-05T16:49:12.564+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on airflow.Home:39821 (size: 66.4 KiB, free: 366.2 MiB)
[2024-01-05T16:49:12.589+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-01-05T16:49:12.591+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T16:49:12.595+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-01-05T16:49:12.611+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 5134 bytes) taskResourceAssignments Map()
[2024-01-05T16:49:12.616+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-01-05T16:49:12.910+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T16:49:12.915+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T16:49:12.958+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T16:49:13.690+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:13 INFO CodeGenerator: Code generated in 98.073842 ms
[2024-01-05T16:49:13.697+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:13 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-01/%202024-01-01.json, range: 0-18085, partition values: [empty row]
[2024-01-05T16:49:13.768+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:13 INFO CodeGenerator: Code generated in 67.203199 ms
[2024-01-05T16:49:13.882+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:13 INFO CodeGenerator: Code generated in 23.223802 ms
[2024-01-05T16:49:14.172+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO FileOutputCommitter: Saved output of task 'attempt_202401051649125198284761500041624_0001_m_000000_1' to file:/home/eder/Desktop/Airflow/Twitter/data/processed/tweets/2024-01-01/_temporary/0/task_202401051649125198284761500041624_0001_m_000000
[2024-01-05T16:49:14.181+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO SparkHadoopMapRedUtil: attempt_202401051649125198284761500041624_0001_m_000000_1: Committed
[2024-01-05T16:49:14.223+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-01-05T16:49:14.266+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1670 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T16:49:14.266+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-01-05T16:49:14.282+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 2.101 s
[2024-01-05T16:49:14.300+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T16:49:14.300+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-01-05T16:49:14.300+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 2.162507 s
[2024-01-05T16:49:14.520+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO FileFormatWriter: Write Job 8df61534-99a6-43ee-a124-a029513535b3 committed.
[2024-01-05T16:49:14.562+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO FileFormatWriter: Finished processing stats for write job 8df61534-99a6-43ee-a124-a029513535b3.
[2024-01-05T16:49:14.712+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO FileSourceStrategy: Pushed Filters:
[2024-01-05T16:49:14.714+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO FileSourceStrategy: Post-Scan Filters:
[2024-01-05T16:49:14.715+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-01-05T16:49:14.759+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T16:49:14.759+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T16:49:14.760+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T16:49:14.975+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:14 INFO CodeGenerator: Code generated in 56.297537 ms
[2024-01-05T16:49:15.022+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-01-05T16:49:15.048+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-01-05T16:49:15.077+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on airflow.Home:39821 (size: 27.5 KiB, free: 366.1 MiB)
[2024-01-05T16:49:15.081+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T16:49:15.082+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212389 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T16:49:15.139+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T16:49:15.142+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T16:49:15.142+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T16:49:15.142+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T16:49:15.142+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO DAGScheduler: Missing parents: List()
[2024-01-05T16:49:15.183+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T16:49:15.517+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.0 KiB, free 364.9 MiB)
[2024-01-05T16:49:15.539+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 63.8 KiB, free 364.8 MiB)
[2024-01-05T16:49:15.543+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on airflow.Home:39821 (size: 63.8 KiB, free: 366.1 MiB)
[2024-01-05T16:49:15.546+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-01-05T16:49:15.548+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T16:49:15.550+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-01-05T16:49:15.556+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 5134 bytes) taskResourceAssignments Map()
[2024-01-05T16:49:15.571+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-01-05T16:49:15.584+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T16:49:15.585+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T16:49:15.585+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T16:49:16.079+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO CodeGenerator: Code generated in 37.316206 ms
[2024-01-05T16:49:16.128+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-01/%202024-01-01.json, range: 0-18085, partition values: [empty row]
[2024-01-05T16:49:16.199+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO CodeGenerator: Code generated in 59.485677 ms
[2024-01-05T16:49:16.249+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO FileOutputCommitter: Saved output of task 'attempt_202401051649153738305994294705583_0002_m_000000_2' to file:/home/eder/Desktop/Airflow/Twitter/data/processed/users/2024-01-01/_temporary/0/task_202401051649153738305994294705583_0002_m_000000
[2024-01-05T16:49:16.264+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO SparkHadoopMapRedUtil: attempt_202401051649153738305994294705583_0002_m_000000_2: Committed
[2024-01-05T16:49:16.270+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-01-05T16:49:16.280+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 728 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T16:49:16.291+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-01-05T16:49:16.291+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 1.097 s
[2024-01-05T16:49:16.291+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T16:49:16.299+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-01-05T16:49:16.318+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 1.170456 s
[2024-01-05T16:49:16.536+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO FileFormatWriter: Write Job 163c6fee-0fc0-4994-ae77-6812378fa983 committed.
[2024-01-05T16:49:16.542+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO FileFormatWriter: Finished processing stats for write job 163c6fee-0fc0-4994-ae77-6812378fa983.
[2024-01-05T16:49:16.922+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:16 INFO SparkContext: Invoking stop() from shutdown hook
[2024-01-05T16:49:17.016+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:17 INFO SparkUI: Stopped Spark web UI at http://airflow.Home:4040
[2024-01-05T16:49:17.171+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-01-05T16:49:17.380+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:17 INFO MemoryStore: MemoryStore cleared
[2024-01-05T16:49:17.380+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:17 INFO BlockManager: BlockManager stopped
[2024-01-05T16:49:17.460+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:17 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-01-05T16:49:17.504+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-01-05T16:49:17.565+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:17 INFO SparkContext: Successfully stopped SparkContext
[2024-01-05T16:49:17.567+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:17 INFO ShutdownHookManager: Shutdown hook called
[2024-01-05T16:49:17.568+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-62c2a1f5-c9ff-4fd6-a8ff-ebbc5b2907ab/pyspark-a169938c-9a7e-402a-9d8c-77cbd3735693
[2024-01-05T16:49:17.591+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-62c2a1f5-c9ff-4fd6-a8ff-ebbc5b2907ab
[2024-01-05T16:49:17.602+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-d42f962e-b19d-407c-ad09-ed5a43bdfe1a
[2024-01-05T16:49:17.783+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_data, execution_date=20240101T000000, start_date=20240105T164832, end_date=20240105T164917
[2024-01-05T16:49:17.943+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-01-05T16:49:17.961+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-05T17:44:50.199+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-01T00:00:00+00:00 [queued]>
[2024-01-05T17:44:50.232+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-01T00:00:00+00:00 [queued]>
[2024-01-05T17:44:50.235+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 1
[2024-01-05T17:44:50.341+0000] {taskinstance.py:2192} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_data> on 2024-01-01 00:00:00+00:00
[2024-01-05T17:44:50.346+0000] {standard_task_runner.py:60} INFO - Started process 26296 to run task
[2024-01-05T17:44:50.348+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_data', 'scheduled__2024-01-01T00:00:00+00:00', '--job-id', '71', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp7ivapmu5']
[2024-01-05T17:44:50.350+0000] {standard_task_runner.py:88} INFO - Job 71: Subtask transform_twitter_data
[2024-01-05T17:44:50.405+0000] {task_command.py:423} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-01T00:00:00+00:00 [running]> on host airflow.vm.local
[2024-01-05T17:44:50.511+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_data' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T00:00:00+00:00'
[2024-01-05T17:44:50.518+0000] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-05T17:44:50.519+0000] {spark_submit.py:366} INFO - Spark-Submit cmd: spark-submit --master local --name Twitter Transformation --queue root.default /home/eder/Desktop/Airflow/Twitter/src/spark/transformation.py --src /home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-01 --dest /home/eder/Desktop/Airflow/Twitter/data/processed/ --process_date 2024-01-01
[2024-01-05T17:44:54.798+0000] {spark_submit.py:536} INFO - 24/01/05 17:44:54 WARN Utils: Your hostname, airflow resolves to a loopback address: 127.0.1.1; using 192.168.1.79 instead (on interface enp0s3)
[2024-01-05T17:44:54.802+0000] {spark_submit.py:536} INFO - 24/01/05 17:44:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-01-05T17:44:58.436+0000] {spark_submit.py:536} INFO - 24/01/05 17:44:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-05T17:45:01.666+0000] {spark_submit.py:536} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-01-05T17:45:01.793+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:01 INFO SparkContext: Running Spark version 3.1.3
[2024-01-05T17:45:02.036+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:02 INFO ResourceUtils: ==============================================================
[2024-01-05T17:45:02.046+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:02 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-05T17:45:02.054+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:02 INFO ResourceUtils: ==============================================================
[2024-01-05T17:45:02.058+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:02 INFO SparkContext: Submitted application: Data Transformation
[2024-01-05T17:45:02.397+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-05T17:45:02.476+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:02 INFO ResourceProfile: Limiting resource is cpu
[2024-01-05T17:45:02.478+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-05T17:45:02.993+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:02 INFO SecurityManager: Changing view acls to: eder
[2024-01-05T17:45:02.994+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:02 INFO SecurityManager: Changing modify acls to: eder
[2024-01-05T17:45:02.994+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:02 INFO SecurityManager: Changing view acls groups to:
[2024-01-05T17:45:03.004+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:03 INFO SecurityManager: Changing modify acls groups to:
[2024-01-05T17:45:03.004+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(eder); groups with view permissions: Set(); users  with modify permissions: Set(eder); groups with modify permissions: Set()
[2024-01-05T17:45:04.931+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:04 INFO Utils: Successfully started service 'sparkDriver' on port 37023.
[2024-01-05T17:45:05.277+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:05 INFO SparkEnv: Registering MapOutputTracker
[2024-01-05T17:45:05.484+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:05 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-05T17:45:05.716+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-05T17:45:05.716+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-05T17:45:05.816+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-05T17:45:06.004+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-eb3d64e7-04f9-4ad4-bb1b-e5dd0298d75e
[2024-01-05T17:45:06.336+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:06 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-01-05T17:45:06.674+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:06 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-05T17:45:08.583+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-05T17:45:09.166+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://airflow.Home:4040
[2024-01-05T17:45:10.811+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:10 INFO Executor: Starting executor ID driver on host airflow.Home
[2024-01-05T17:45:11.090+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35645.
[2024-01-05T17:45:11.091+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:11 INFO NettyBlockTransferService: Server created on airflow.Home:35645
[2024-01-05T17:45:11.106+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-05T17:45:11.176+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, airflow.Home, 35645, None)
[2024-01-05T17:45:11.362+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:11 INFO BlockManagerMasterEndpoint: Registering block manager airflow.Home:35645 with 366.3 MiB RAM, BlockManagerId(driver, airflow.Home, 35645, None)
[2024-01-05T17:45:11.389+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, airflow.Home, 35645, None)
[2024-01-05T17:45:11.395+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, airflow.Home, 35645, None)
[2024-01-05T17:45:15.460+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/eder/Desktop/Airflow/Twitter/spark-warehouse').
[2024-01-05T17:45:15.491+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:15 INFO SharedState: Warehouse path is 'file:/home/eder/Desktop/Airflow/Twitter/spark-warehouse'.
[2024-01-05T17:45:20.845+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:20 INFO InMemoryFileIndex: It took 256 ms to list leaf files for 1 paths.
[2024-01-05T17:45:21.484+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:21 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
[2024-01-05T17:45:35.742+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:35 INFO FileSourceStrategy: Pushed Filters:
[2024-01-05T17:45:35.748+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:35 INFO FileSourceStrategy: Post-Scan Filters:
[2024-01-05T17:45:35.785+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:35 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-01-05T17:45:38.250+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-01-05T17:45:38.986+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 366.0 MiB)
[2024-01-05T17:45:39.032+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on airflow.Home:35645 (size: 27.4 KiB, free: 366.3 MiB)
[2024-01-05T17:45:39.065+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:39 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T17:45:39.130+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198751 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T17:45:40.488+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:40 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T17:45:40.695+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:40 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T17:45:40.704+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:40 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T17:45:40.716+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:40 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T17:45:40.749+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:40 INFO DAGScheduler: Missing parents: List()
[2024-01-05T17:45:40.863+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T17:45:41.814+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 366.0 MiB)
[2024-01-05T17:45:41.896+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-01-05T17:45:41.914+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on airflow.Home:35645 (size: 6.3 KiB, free: 366.3 MiB)
[2024-01-05T17:45:41.950+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-01-05T17:45:42.029+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T17:45:42.038+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-01-05T17:45:42.782+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
[2024-01-05T17:45:42.986+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-01-05T17:45:45.472+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:45 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-01/%202024-01-01.json, range: 0-4447, partition values: [empty row]
[2024-01-05T17:45:48.259+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:48 INFO CodeGenerator: Code generated in 1792.216464 ms
[2024-01-05T17:45:49.106+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-01-05T17:45:49.286+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6694 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T17:45:49.339+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-01-05T17:45:49.371+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:49 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 8.204 s
[2024-01-05T17:45:49.448+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T17:45:49.460+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-01-05T17:45:49.479+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:49 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 8.986278 s
[2024-01-05T17:45:52.895+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:52 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-01-05T17:45:52.926+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:52 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-01-05T17:45:52.929+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:52 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-01-05T17:45:53.680+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T17:45:53.699+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T17:45:53.716+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T17:45:54.595+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:54 INFO CodeGenerator: Code generated in 158.409571 ms
[2024-01-05T17:45:54.785+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:54 INFO CodeGenerator: Code generated in 86.625337 ms
[2024-01-05T17:45:54.800+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:54 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-01-05T17:45:54.825+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:54 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-01-05T17:45:54.827+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on airflow.Home:35645 (size: 27.5 KiB, free: 366.2 MiB)
[2024-01-05T17:45:54.831+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:54 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T17:45:54.838+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198751 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T17:45:55.060+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:55 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T17:45:55.077+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:55 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T17:45:55.078+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:55 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T17:45:55.078+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:55 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T17:45:55.078+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:55 INFO DAGScheduler: Missing parents: List()
[2024-01-05T17:45:55.083+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:55 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T17:45:55.721+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 189.7 KiB, free 365.5 MiB)
[2024-01-05T17:45:55.726+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.4 KiB, free 365.4 MiB)
[2024-01-05T17:45:55.728+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on airflow.Home:35645 (size: 66.4 KiB, free: 366.2 MiB)
[2024-01-05T17:45:55.731+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-01-05T17:45:55.733+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T17:45:55.733+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-01-05T17:45:55.744+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 5134 bytes) taskResourceAssignments Map()
[2024-01-05T17:45:55.746+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-01-05T17:45:56.093+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T17:45:56.100+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T17:45:56.118+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T17:45:57.076+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:57 INFO CodeGenerator: Code generated in 105.011048 ms
[2024-01-05T17:45:57.098+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:57 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-01/%202024-01-01.json, range: 0-4447, partition values: [empty row]
[2024-01-05T17:45:57.199+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:57 INFO CodeGenerator: Code generated in 89.800902 ms
[2024-01-05T17:45:57.376+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:57 INFO CodeGenerator: Code generated in 41.886638 ms
[2024-01-05T17:45:57.679+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:57 INFO FileOutputCommitter: Saved output of task 'attempt_202401051745543456560332906995986_0001_m_000000_1' to file:/home/eder/Desktop/Airflow/Twitter/data/processed/tweets/2024-01-01/_temporary/0/task_202401051745543456560332906995986_0001_m_000000
[2024-01-05T17:45:57.688+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:57 INFO SparkHadoopMapRedUtil: attempt_202401051745543456560332906995986_0001_m_000000_1: Committed
[2024-01-05T17:45:57.726+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:57 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-01-05T17:45:57.750+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2009 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T17:45:57.765+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:57 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 2.675 s
[2024-01-05T17:45:57.768+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:57 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T17:45:57.788+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-01-05T17:45:57.790+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-01-05T17:45:57.790+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:57 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 2.709870 s
[2024-01-05T17:45:57.998+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:57 INFO FileFormatWriter: Write Job 7bafa7a7-e2fc-428e-9575-1d9349596f8b committed.
[2024-01-05T17:45:58.029+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO FileFormatWriter: Finished processing stats for write job 7bafa7a7-e2fc-428e-9575-1d9349596f8b.
[2024-01-05T17:45:58.235+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO FileSourceStrategy: Pushed Filters:
[2024-01-05T17:45:58.235+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO FileSourceStrategy: Post-Scan Filters:
[2024-01-05T17:45:58.236+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-01-05T17:45:58.286+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T17:45:58.287+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T17:45:58.287+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T17:45:58.530+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO CodeGenerator: Code generated in 86.649362 ms
[2024-01-05T17:45:58.545+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-01-05T17:45:58.568+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-01-05T17:45:58.575+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on airflow.Home:35645 (size: 27.5 KiB, free: 366.1 MiB)
[2024-01-05T17:45:58.575+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T17:45:58.577+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198751 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T17:45:58.665+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T17:45:58.670+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T17:45:58.676+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T17:45:58.677+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T17:45:58.677+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO DAGScheduler: Missing parents: List()
[2024-01-05T17:45:58.679+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:58 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T17:45:59.004+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.0 KiB, free 364.9 MiB)
[2024-01-05T17:45:59.011+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 63.8 KiB, free 364.8 MiB)
[2024-01-05T17:45:59.015+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on airflow.Home:35645 (size: 63.8 KiB, free: 366.1 MiB)
[2024-01-05T17:45:59.027+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-01-05T17:45:59.029+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T17:45:59.029+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-01-05T17:45:59.034+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 5134 bytes) taskResourceAssignments Map()
[2024-01-05T17:45:59.035+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-01-05T17:45:59.088+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T17:45:59.093+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T17:45:59.094+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T17:45:59.494+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO CodeGenerator: Code generated in 68.58101 ms
[2024-01-05T17:45:59.506+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-01/%202024-01-01.json, range: 0-4447, partition values: [empty row]
[2024-01-05T17:45:59.563+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO CodeGenerator: Code generated in 49.750545 ms
[2024-01-05T17:45:59.645+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO FileOutputCommitter: Saved output of task 'attempt_20240105174558948166161477592209_0002_m_000000_2' to file:/home/eder/Desktop/Airflow/Twitter/data/processed/users/2024-01-01/_temporary/0/task_20240105174558948166161477592209_0002_m_000000
[2024-01-05T17:45:59.646+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO SparkHadoopMapRedUtil: attempt_20240105174558948166161477592209_0002_m_000000_2: Committed
[2024-01-05T17:45:59.650+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-01-05T17:45:59.658+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 624 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T17:45:59.658+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-01-05T17:45:59.659+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.937 s
[2024-01-05T17:45:59.661+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T17:45:59.662+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-01-05T17:45:59.663+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.994635 s
[2024-01-05T17:45:59.940+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO FileFormatWriter: Write Job 23f82f08-4bb5-4bcd-82b8-4fd4ed656260 committed.
[2024-01-05T17:45:59.966+0000] {spark_submit.py:536} INFO - 24/01/05 17:45:59 INFO FileFormatWriter: Finished processing stats for write job 23f82f08-4bb5-4bcd-82b8-4fd4ed656260.
[2024-01-05T17:46:00.518+0000] {spark_submit.py:536} INFO - 24/01/05 17:46:00 INFO SparkContext: Invoking stop() from shutdown hook
[2024-01-05T17:46:00.759+0000] {spark_submit.py:536} INFO - 24/01/05 17:46:00 INFO SparkUI: Stopped Spark web UI at http://airflow.Home:4040
[2024-01-05T17:46:00.877+0000] {spark_submit.py:536} INFO - 24/01/05 17:46:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on airflow.Home:35645 in memory (size: 27.5 KiB, free: 366.1 MiB)
[2024-01-05T17:46:01.290+0000] {spark_submit.py:536} INFO - 24/01/05 17:46:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-01-05T17:46:01.484+0000] {spark_submit.py:536} INFO - 24/01/05 17:46:01 INFO MemoryStore: MemoryStore cleared
[2024-01-05T17:46:01.486+0000] {spark_submit.py:536} INFO - 24/01/05 17:46:01 INFO BlockManager: BlockManager stopped
[2024-01-05T17:46:01.510+0000] {spark_submit.py:536} INFO - 24/01/05 17:46:01 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-01-05T17:46:01.584+0000] {spark_submit.py:536} INFO - 24/01/05 17:46:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-01-05T17:46:01.715+0000] {spark_submit.py:536} INFO - 24/01/05 17:46:01 INFO SparkContext: Successfully stopped SparkContext
[2024-01-05T17:46:01.719+0000] {spark_submit.py:536} INFO - 24/01/05 17:46:01 INFO ShutdownHookManager: Shutdown hook called
[2024-01-05T17:46:01.733+0000] {spark_submit.py:536} INFO - 24/01/05 17:46:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-898a0f95-03a7-4904-a711-8537fbf28f70
[2024-01-05T17:46:01.752+0000] {spark_submit.py:536} INFO - 24/01/05 17:46:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-e8839492-abc1-483f-9875-644b62f5e0a5/pyspark-704f9180-6143-49bf-82a2-3ea9d0709f0b
[2024-01-05T17:46:01.776+0000] {spark_submit.py:536} INFO - 24/01/05 17:46:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-e8839492-abc1-483f-9875-644b62f5e0a5
[2024-01-05T17:46:02.162+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_data, execution_date=20240101T000000, start_date=20240105T174450, end_date=20240105T174602
[2024-01-05T17:46:02.767+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-01-05T17:46:02.862+0000] {taskinstance.py:3281} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-01-05T18:04:36.915+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-01T00:00:00+00:00 [queued]>
[2024-01-05T18:04:36.926+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-01T00:00:00+00:00 [queued]>
[2024-01-05T18:04:36.926+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 1
[2024-01-05T18:04:36.966+0000] {taskinstance.py:2192} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_data> on 2024-01-01 00:00:00+00:00
[2024-01-05T18:04:36.972+0000] {standard_task_runner.py:60} INFO - Started process 37878 to run task
[2024-01-05T18:04:36.976+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_data', 'scheduled__2024-01-01T00:00:00+00:00', '--job-id', '94', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpwqta__ya']
[2024-01-05T18:04:36.978+0000] {standard_task_runner.py:88} INFO - Job 94: Subtask transform_twitter_data
[2024-01-05T18:04:37.130+0000] {task_command.py:423} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-01T00:00:00+00:00 [running]> on host airflow.vm.local
[2024-01-05T18:04:37.278+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_data' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T00:00:00+00:00'
[2024-01-05T18:04:37.282+0000] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-05T18:04:37.283+0000] {spark_submit.py:366} INFO - Spark-Submit cmd: spark-submit --master local --name Twitter Transformation --queue root.default /home/eder/Desktop/Airflow/Twitter/src/spark/transformation.py --src /home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-01 --dest /home/eder/Desktop/Airflow/Twitter/data/processed/ --process_date 2024-01-01
[2024-01-05T18:04:39.974+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:39 WARN Utils: Your hostname, airflow resolves to a loopback address: 127.0.1.1; using 192.168.1.79 instead (on interface enp0s3)
[2024-01-05T18:04:39.974+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-01-05T18:04:40.967+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-05T18:04:41.947+0000] {spark_submit.py:536} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-01-05T18:04:41.990+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:41 INFO SparkContext: Running Spark version 3.1.3
[2024-01-05T18:04:42.191+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:42 INFO ResourceUtils: ==============================================================
[2024-01-05T18:04:42.199+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:42 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-05T18:04:42.211+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:42 INFO ResourceUtils: ==============================================================
[2024-01-05T18:04:42.215+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:42 INFO SparkContext: Submitted application: Data Transformation
[2024-01-05T18:04:42.331+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-05T18:04:42.356+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:42 INFO ResourceProfile: Limiting resource is cpu
[2024-01-05T18:04:42.370+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:42 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-05T18:04:42.501+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:42 INFO SecurityManager: Changing view acls to: eder
[2024-01-05T18:04:42.502+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:42 INFO SecurityManager: Changing modify acls to: eder
[2024-01-05T18:04:42.502+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:42 INFO SecurityManager: Changing view acls groups to:
[2024-01-05T18:04:42.502+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:42 INFO SecurityManager: Changing modify acls groups to:
[2024-01-05T18:04:42.502+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(eder); groups with view permissions: Set(); users  with modify permissions: Set(eder); groups with modify permissions: Set()
[2024-01-05T18:04:43.205+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:43 INFO Utils: Successfully started service 'sparkDriver' on port 41695.
[2024-01-05T18:04:43.303+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:43 INFO SparkEnv: Registering MapOutputTracker
[2024-01-05T18:04:43.406+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:43 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-05T18:04:43.455+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-05T18:04:43.456+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-05T18:04:43.462+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-05T18:04:43.487+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e4ef1143-90d5-4588-9064-d82aa3e01909
[2024-01-05T18:04:43.535+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-01-05T18:04:43.599+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:43 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-05T18:04:44.195+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-05T18:04:44.315+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://airflow.Home:4040
[2024-01-05T18:04:45.227+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:45 INFO Executor: Starting executor ID driver on host airflow.Home
[2024-01-05T18:04:45.314+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34239.
[2024-01-05T18:04:45.323+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:45 INFO NettyBlockTransferService: Server created on airflow.Home:34239
[2024-01-05T18:04:45.324+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-05T18:04:45.345+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, airflow.Home, 34239, None)
[2024-01-05T18:04:45.353+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:45 INFO BlockManagerMasterEndpoint: Registering block manager airflow.Home:34239 with 366.3 MiB RAM, BlockManagerId(driver, airflow.Home, 34239, None)
[2024-01-05T18:04:45.361+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, airflow.Home, 34239, None)
[2024-01-05T18:04:45.362+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, airflow.Home, 34239, None)
[2024-01-05T18:04:46.504+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:46 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/eder/Desktop/Airflow/Twitter/spark-warehouse').
[2024-01-05T18:04:46.506+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:46 INFO SharedState: Warehouse path is 'file:/home/eder/Desktop/Airflow/Twitter/spark-warehouse'.
[2024-01-05T18:04:49.379+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:49 INFO InMemoryFileIndex: It took 109 ms to list leaf files for 1 paths.
[2024-01-05T18:04:49.534+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:49 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-01-05T18:04:54.116+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:54 INFO FileSourceStrategy: Pushed Filters:
[2024-01-05T18:04:54.118+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:54 INFO FileSourceStrategy: Post-Scan Filters:
[2024-01-05T18:04:54.136+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:54 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-01-05T18:04:54.950+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-01-05T18:04:55.050+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 366.0 MiB)
[2024-01-05T18:04:55.055+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on airflow.Home:34239 (size: 27.4 KiB, free: 366.3 MiB)
[2024-01-05T18:04:55.067+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T18:04:55.084+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198842 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T18:04:55.431+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T18:04:55.473+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T18:04:55.474+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T18:04:55.475+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T18:04:55.491+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO DAGScheduler: Missing parents: List()
[2024-01-05T18:04:55.511+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T18:04:55.796+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 366.0 MiB)
[2024-01-05T18:04:55.813+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-01-05T18:04:55.813+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on airflow.Home:34239 (size: 6.3 KiB, free: 366.3 MiB)
[2024-01-05T18:04:55.815+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-01-05T18:04:55.850+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T18:04:55.874+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-01-05T18:04:56.030+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
[2024-01-05T18:04:56.061+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-01-05T18:04:56.869+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:56 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-01/%202024-01-01.json, range: 0-4538, partition values: [empty row]
[2024-01-05T18:04:57.535+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:57 INFO CodeGenerator: Code generated in 424.909447 ms
[2024-01-05T18:04:57.678+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-01-05T18:04:57.846+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1828 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T18:04:57.856+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-01-05T18:04:57.875+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:57 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 2.320 s
[2024-01-05T18:04:58.078+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T18:04:58.078+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-01-05T18:04:58.086+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:58 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 2.653465 s
[2024-01-05T18:04:58.832+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:58 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-01-05T18:04:58.835+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:58 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-01-05T18:04:58.835+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:58 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-01-05T18:04:58.983+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T18:04:58.983+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T18:04:58.984+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T18:04:59.258+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO CodeGenerator: Code generated in 59.74496 ms
[2024-01-05T18:04:59.379+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO CodeGenerator: Code generated in 48.68709 ms
[2024-01-05T18:04:59.387+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-01-05T18:04:59.395+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-01-05T18:04:59.400+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on airflow.Home:34239 (size: 27.5 KiB, free: 366.2 MiB)
[2024-01-05T18:04:59.405+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T18:04:59.410+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198842 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T18:04:59.516+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T18:04:59.518+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T18:04:59.519+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T18:04:59.519+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T18:04:59.519+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO DAGScheduler: Missing parents: List()
[2024-01-05T18:04:59.523+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T18:04:59.726+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 189.7 KiB, free 365.5 MiB)
[2024-01-05T18:04:59.729+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.4 KiB, free 365.4 MiB)
[2024-01-05T18:04:59.729+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on airflow.Home:34239 (size: 66.4 KiB, free: 366.2 MiB)
[2024-01-05T18:04:59.730+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-01-05T18:04:59.732+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T18:04:59.732+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-01-05T18:04:59.738+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 5134 bytes) taskResourceAssignments Map()
[2024-01-05T18:04:59.746+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-01-05T18:04:59.843+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T18:04:59.843+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T18:04:59.844+0000] {spark_submit.py:536} INFO - 24/01/05 18:04:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T18:05:00.104+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO CodeGenerator: Code generated in 42.791068 ms
[2024-01-05T18:05:00.111+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-01/%202024-01-01.json, range: 0-4538, partition values: [empty row]
[2024-01-05T18:05:00.148+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO CodeGenerator: Code generated in 33.30575 ms
[2024-01-05T18:05:00.191+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO CodeGenerator: Code generated in 11.24728 ms
[2024-01-05T18:05:00.266+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO FileOutputCommitter: Saved output of task 'attempt_202401051804592913172601277713721_0001_m_000000_1' to file:/home/eder/Desktop/Airflow/Twitter/data/processed/tweets/2024-01-01/_temporary/0/task_202401051804592913172601277713721_0001_m_000000
[2024-01-05T18:05:00.266+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO SparkHadoopMapRedUtil: attempt_202401051804592913172601277713721_0001_m_000000_1: Committed
[2024-01-05T18:05:00.273+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-01-05T18:05:00.277+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 543 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T18:05:00.278+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-01-05T18:05:00.278+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.754 s
[2024-01-05T18:05:00.279+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T18:05:00.279+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-01-05T18:05:00.295+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.779485 s
[2024-01-05T18:05:00.367+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO FileFormatWriter: Write Job 498b8b90-7449-422d-b1bd-22be1484b526 committed.
[2024-01-05T18:05:00.380+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO FileFormatWriter: Finished processing stats for write job 498b8b90-7449-422d-b1bd-22be1484b526.
[2024-01-05T18:05:00.490+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO FileSourceStrategy: Pushed Filters:
[2024-01-05T18:05:00.490+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO FileSourceStrategy: Post-Scan Filters:
[2024-01-05T18:05:00.490+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-01-05T18:05:00.502+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T18:05:00.502+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T18:05:00.503+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T18:05:00.671+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO CodeGenerator: Code generated in 24.498733 ms
[2024-01-05T18:05:00.684+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-01-05T18:05:00.694+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-01-05T18:05:00.696+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on airflow.Home:34239 (size: 27.5 KiB, free: 366.1 MiB)
[2024-01-05T18:05:00.699+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T18:05:00.703+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198842 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T18:05:00.748+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T18:05:00.751+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T18:05:00.751+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T18:05:00.751+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T18:05:00.751+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO DAGScheduler: Missing parents: List()
[2024-01-05T18:05:00.757+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T18:05:00.864+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.0 KiB, free 364.9 MiB)
[2024-01-05T18:05:00.871+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 63.8 KiB, free 364.8 MiB)
[2024-01-05T18:05:00.872+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on airflow.Home:34239 (size: 63.8 KiB, free: 366.1 MiB)
[2024-01-05T18:05:00.875+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-01-05T18:05:00.875+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T18:05:00.876+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-01-05T18:05:00.877+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 5134 bytes) taskResourceAssignments Map()
[2024-01-05T18:05:00.878+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-01-05T18:05:00.919+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T18:05:00.919+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T18:05:00.919+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T18:05:00.991+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:00 INFO BlockManagerInfo: Removed broadcast_3_piece0 on airflow.Home:34239 in memory (size: 66.4 KiB, free: 366.2 MiB)
[2024-01-05T18:05:01.013+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO CodeGenerator: Code generated in 25.687436 ms
[2024-01-05T18:05:01.043+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO BlockManagerInfo: Removed broadcast_2_piece0 on airflow.Home:34239 in memory (size: 27.5 KiB, free: 366.2 MiB)
[2024-01-05T18:05:01.062+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-01/%202024-01-01.json, range: 0-4538, partition values: [empty row]
[2024-01-05T18:05:01.111+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO CodeGenerator: Code generated in 31.989936 ms
[2024-01-05T18:05:01.123+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO FileOutputCommitter: Saved output of task 'attempt_202401051805004809340118572333959_0002_m_000000_2' to file:/home/eder/Desktop/Airflow/Twitter/data/processed/users/2024-01-01/_temporary/0/task_202401051805004809340118572333959_0002_m_000000
[2024-01-05T18:05:01.124+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO SparkHadoopMapRedUtil: attempt_202401051805004809340118572333959_0002_m_000000_2: Committed
[2024-01-05T18:05:01.127+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2777 bytes result sent to driver
[2024-01-05T18:05:01.132+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 255 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T18:05:01.133+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-01-05T18:05:01.134+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.378 s
[2024-01-05T18:05:01.134+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T18:05:01.134+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-01-05T18:05:01.137+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.387920 s
[2024-01-05T18:05:01.251+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO FileFormatWriter: Write Job 98d7e9a8-cbb2-4ba6-9f42-f0e461c31d94 committed.
[2024-01-05T18:05:01.252+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO FileFormatWriter: Finished processing stats for write job 98d7e9a8-cbb2-4ba6-9f42-f0e461c31d94.
[2024-01-05T18:05:01.420+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO SparkContext: Invoking stop() from shutdown hook
[2024-01-05T18:05:01.473+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO SparkUI: Stopped Spark web UI at http://airflow.Home:4040
[2024-01-05T18:05:01.558+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-01-05T18:05:01.581+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO MemoryStore: MemoryStore cleared
[2024-01-05T18:05:01.582+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO BlockManager: BlockManager stopped
[2024-01-05T18:05:01.663+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-01-05T18:05:01.682+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-01-05T18:05:01.692+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO SparkContext: Successfully stopped SparkContext
[2024-01-05T18:05:01.693+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO ShutdownHookManager: Shutdown hook called
[2024-01-05T18:05:01.694+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-c2664b7a-3556-4793-b169-be88fe7212bf/pyspark-8924e5b9-3486-4046-a237-9eae1be304a6
[2024-01-05T18:05:01.702+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-c2664b7a-3556-4793-b169-be88fe7212bf
[2024-01-05T18:05:01.709+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-7fe28d10-1286-4f6f-8719-d9f3f1e8b0e6
[2024-01-05T18:05:01.840+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_data, execution_date=20240101T000000, start_date=20240105T180436, end_date=20240105T180501
[2024-01-05T18:05:02.009+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-01-05T18:05:02.032+0000] {taskinstance.py:3281} INFO - 1 downstream tasks scheduled from follow-on schedule check
