[2024-01-05T16:49:40.964+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-02T00:00:00+00:00 [queued]>
[2024-01-05T16:49:40.980+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-02T00:00:00+00:00 [queued]>
[2024-01-05T16:49:40.988+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 1
[2024-01-05T16:49:41.033+0000] {taskinstance.py:2192} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_data> on 2024-01-02 00:00:00+00:00
[2024-01-05T16:49:41.043+0000] {standard_task_runner.py:60} INFO - Started process 13751 to run task
[2024-01-05T16:49:41.049+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_data', 'scheduled__2024-01-02T00:00:00+00:00', '--job-id', '52', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmprm320hy0']
[2024-01-05T16:49:41.050+0000] {standard_task_runner.py:88} INFO - Job 52: Subtask transform_twitter_data
[2024-01-05T16:49:41.204+0000] {task_command.py:423} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-02T00:00:00+00:00 [running]> on host airflow.vm.local
[2024-01-05T16:49:41.496+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_data' AIRFLOW_CTX_EXECUTION_DATE='2024-01-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-02T00:00:00+00:00'
[2024-01-05T16:49:41.531+0000] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-05T16:49:41.532+0000] {spark_submit.py:366} INFO - Spark-Submit cmd: spark-submit --master local --name Twitter Transformation --queue root.default /home/eder/Desktop/Airflow/Twitter/src/spark/transformation.py --src /home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-02 --dest /home/eder/Desktop/Airflow/Twitter/data/processed/ --process_date 2024-01-02
[2024-01-05T16:49:45.069+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:45 WARN Utils: Your hostname, airflow resolves to a loopback address: 127.0.1.1; using 192.168.1.79 instead (on interface enp0s3)
[2024-01-05T16:49:45.070+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-01-05T16:49:46.207+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-05T16:49:47.936+0000] {spark_submit.py:536} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-01-05T16:49:47.983+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:47 INFO SparkContext: Running Spark version 3.1.3
[2024-01-05T16:49:48.142+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:48 INFO ResourceUtils: ==============================================================
[2024-01-05T16:49:48.151+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:48 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-05T16:49:48.151+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:48 INFO ResourceUtils: ==============================================================
[2024-01-05T16:49:48.151+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:48 INFO SparkContext: Submitted application: Data Transformation
[2024-01-05T16:49:48.225+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-05T16:49:48.263+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:48 INFO ResourceProfile: Limiting resource is cpu
[2024-01-05T16:49:48.267+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-05T16:49:48.399+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:48 INFO SecurityManager: Changing view acls to: eder
[2024-01-05T16:49:48.399+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:48 INFO SecurityManager: Changing modify acls to: eder
[2024-01-05T16:49:48.400+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:48 INFO SecurityManager: Changing view acls groups to:
[2024-01-05T16:49:48.400+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:48 INFO SecurityManager: Changing modify acls groups to:
[2024-01-05T16:49:48.400+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(eder); groups with view permissions: Set(); users  with modify permissions: Set(eder); groups with modify permissions: Set()
[2024-01-05T16:49:49.170+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:49 INFO Utils: Successfully started service 'sparkDriver' on port 37573.
[2024-01-05T16:49:49.368+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:49 INFO SparkEnv: Registering MapOutputTracker
[2024-01-05T16:49:49.504+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:49 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-05T16:49:49.558+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-05T16:49:49.566+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-05T16:49:49.574+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-05T16:49:49.634+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e40a571b-711c-43fa-8222-f277623faa3b
[2024-01-05T16:49:49.727+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-01-05T16:49:49.818+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:49 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-05T16:49:50.894+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-05T16:49:51.168+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:51 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://airflow.Home:4040
[2024-01-05T16:49:52.322+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:52 INFO Executor: Starting executor ID driver on host airflow.Home
[2024-01-05T16:49:52.536+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36383.
[2024-01-05T16:49:52.536+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:52 INFO NettyBlockTransferService: Server created on airflow.Home:36383
[2024-01-05T16:49:52.551+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-05T16:49:52.588+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, airflow.Home, 36383, None)
[2024-01-05T16:49:52.652+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:52 INFO BlockManagerMasterEndpoint: Registering block manager airflow.Home:36383 with 366.3 MiB RAM, BlockManagerId(driver, airflow.Home, 36383, None)
[2024-01-05T16:49:52.670+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, airflow.Home, 36383, None)
[2024-01-05T16:49:52.679+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, airflow.Home, 36383, None)
[2024-01-05T16:49:55.221+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/eder/Desktop/Airflow/Twitter/spark-warehouse').
[2024-01-05T16:49:55.223+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:55 INFO SharedState: Warehouse path is 'file:/home/eder/Desktop/Airflow/Twitter/spark-warehouse'.
[2024-01-05T16:49:58.556+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:58 INFO InMemoryFileIndex: It took 247 ms to list leaf files for 1 paths.
[2024-01-05T16:49:58.917+0000] {spark_submit.py:536} INFO - 24/01/05 16:49:58 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-01-05T16:50:06.307+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:06 INFO FileSourceStrategy: Pushed Filters:
[2024-01-05T16:50:06.315+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:06 INFO FileSourceStrategy: Post-Scan Filters:
[2024-01-05T16:50:06.329+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:06 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-01-05T16:50:07.367+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-01-05T16:50:07.739+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 366.0 MiB)
[2024-01-05T16:50:07.752+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on airflow.Home:36383 (size: 27.4 KiB, free: 366.3 MiB)
[2024-01-05T16:50:07.767+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:07 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T16:50:07.791+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207942 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T16:50:08.404+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:08 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T16:50:08.483+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:08 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T16:50:08.484+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:08 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T16:50:08.495+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:08 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T16:50:08.503+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:08 INFO DAGScheduler: Missing parents: List()
[2024-01-05T16:50:08.543+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T16:50:09.134+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 366.0 MiB)
[2024-01-05T16:50:09.163+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-01-05T16:50:09.173+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on airflow.Home:36383 (size: 6.3 KiB, free: 366.3 MiB)
[2024-01-05T16:50:09.203+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:09 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-01-05T16:50:09.288+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T16:50:09.315+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-01-05T16:50:09.546+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
[2024-01-05T16:50:09.664+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-01-05T16:50:10.312+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:10 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-02/%202024-01-02.json, range: 0-13638, partition values: [empty row]
[2024-01-05T16:50:11.721+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:11 INFO CodeGenerator: Code generated in 899.057296 ms
[2024-01-05T16:50:12.455+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-01-05T16:50:12.479+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2952 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T16:50:12.487+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-01-05T16:50:12.498+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:12 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 3.818 s
[2024-01-05T16:50:12.515+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:12 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T16:50:12.523+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-01-05T16:50:12.528+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:12 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 4.121915 s
[2024-01-05T16:50:14.148+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:14 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-01-05T16:50:14.164+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:14 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-01-05T16:50:14.165+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:14 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-01-05T16:50:14.508+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T16:50:14.522+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T16:50:14.524+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T16:50:15.208+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:15 INFO CodeGenerator: Code generated in 117.031148 ms
[2024-01-05T16:50:15.407+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:15 INFO CodeGenerator: Code generated in 103.748072 ms
[2024-01-05T16:50:15.418+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-01-05T16:50:15.431+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-01-05T16:50:15.440+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on airflow.Home:36383 (size: 27.5 KiB, free: 366.2 MiB)
[2024-01-05T16:50:15.458+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:15 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T16:50:15.458+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207942 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T16:50:15.690+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:15 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T16:50:15.711+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:15 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T16:50:15.721+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:15 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T16:50:15.726+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:15 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T16:50:15.727+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:15 INFO DAGScheduler: Missing parents: List()
[2024-01-05T16:50:15.750+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:15 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T16:50:16.349+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 189.7 KiB, free 365.5 MiB)
[2024-01-05T16:50:16.357+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.3 KiB, free 365.4 MiB)
[2024-01-05T16:50:16.367+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on airflow.Home:36383 (size: 66.3 KiB, free: 366.2 MiB)
[2024-01-05T16:50:16.368+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-01-05T16:50:16.371+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T16:50:16.372+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-01-05T16:50:16.399+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 5134 bytes) taskResourceAssignments Map()
[2024-01-05T16:50:16.403+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-01-05T16:50:16.753+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T16:50:16.758+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T16:50:16.762+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T16:50:17.299+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO CodeGenerator: Code generated in 75.259033 ms
[2024-01-05T16:50:17.334+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-02/%202024-01-02.json, range: 0-13638, partition values: [empty row]
[2024-01-05T16:50:17.424+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO CodeGenerator: Code generated in 80.916221 ms
[2024-01-05T16:50:17.518+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO CodeGenerator: Code generated in 14.520674 ms
[2024-01-05T16:50:17.743+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO FileOutputCommitter: Saved output of task 'attempt_202401051650157970577623352826474_0001_m_000000_1' to file:/home/eder/Desktop/Airflow/Twitter/data/processed/tweets/2024-01-02/_temporary/0/task_202401051650157970577623352826474_0001_m_000000
[2024-01-05T16:50:17.752+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO SparkHadoopMapRedUtil: attempt_202401051650157970577623352826474_0001_m_000000_1: Committed
[2024-01-05T16:50:17.779+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-01-05T16:50:17.781+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1406 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T16:50:17.783+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-01-05T16:50:17.785+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 2.052 s
[2024-01-05T16:50:17.785+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T16:50:17.785+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-01-05T16:50:17.793+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 2.100331 s
[2024-01-05T16:50:17.925+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO FileFormatWriter: Write Job 8684a687-41fd-4715-a8b1-390a166a0202 committed.
[2024-01-05T16:50:17.934+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:17 INFO FileFormatWriter: Finished processing stats for write job 8684a687-41fd-4715-a8b1-390a166a0202.
[2024-01-05T16:50:18.216+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO FileSourceStrategy: Pushed Filters:
[2024-01-05T16:50:18.218+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO FileSourceStrategy: Post-Scan Filters:
[2024-01-05T16:50:18.218+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-01-05T16:50:18.229+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T16:50:18.230+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T16:50:18.230+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T16:50:18.379+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO BlockManagerInfo: Removed broadcast_3_piece0 on airflow.Home:36383 in memory (size: 66.3 KiB, free: 366.2 MiB)
[2024-01-05T16:50:18.552+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO CodeGenerator: Code generated in 51.935217 ms
[2024-01-05T16:50:18.583+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.3 MiB)
[2024-01-05T16:50:18.591+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.3 MiB)
[2024-01-05T16:50:18.604+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on airflow.Home:36383 (size: 27.5 KiB, free: 366.2 MiB)
[2024-01-05T16:50:18.616+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T16:50:18.618+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207942 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T16:50:18.667+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T16:50:18.680+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T16:50:18.680+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T16:50:18.682+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T16:50:18.683+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO DAGScheduler: Missing parents: List()
[2024-01-05T16:50:18.770+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:18 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T16:50:19.088+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.0 KiB, free 365.1 MiB)
[2024-01-05T16:50:19.095+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 63.8 KiB, free 365.1 MiB)
[2024-01-05T16:50:19.099+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on airflow.Home:36383 (size: 63.8 KiB, free: 366.2 MiB)
[2024-01-05T16:50:19.102+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-01-05T16:50:19.134+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T16:50:19.138+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-01-05T16:50:19.138+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 5134 bytes) taskResourceAssignments Map()
[2024-01-05T16:50:19.146+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-01-05T16:50:19.218+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T16:50:19.219+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T16:50:19.219+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T16:50:19.643+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO CodeGenerator: Code generated in 38.662757 ms
[2024-01-05T16:50:19.682+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-02/%202024-01-02.json, range: 0-13638, partition values: [empty row]
[2024-01-05T16:50:19.746+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO CodeGenerator: Code generated in 53.79748 ms
[2024-01-05T16:50:19.789+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO FileOutputCommitter: Saved output of task 'attempt_202401051650187451549758377381908_0002_m_000000_2' to file:/home/eder/Desktop/Airflow/Twitter/data/processed/users/2024-01-02/_temporary/0/task_202401051650187451549758377381908_0002_m_000000
[2024-01-05T16:50:19.791+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO SparkHadoopMapRedUtil: attempt_202401051650187451549758377381908_0002_m_000000_2: Committed
[2024-01-05T16:50:19.803+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2777 bytes result sent to driver
[2024-01-05T16:50:19.823+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 688 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T16:50:19.823+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-01-05T16:50:19.825+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 1.053 s
[2024-01-05T16:50:19.825+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T16:50:19.826+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-01-05T16:50:19.826+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 1.158569 s
[2024-01-05T16:50:19.936+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO FileFormatWriter: Write Job 2a03f29c-14de-488c-831a-27f376eb2431 committed.
[2024-01-05T16:50:19.943+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:19 INFO FileFormatWriter: Finished processing stats for write job 2a03f29c-14de-488c-831a-27f376eb2431.
[2024-01-05T16:50:20.344+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:20 INFO SparkContext: Invoking stop() from shutdown hook
[2024-01-05T16:50:20.452+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:20 INFO SparkUI: Stopped Spark web UI at http://airflow.Home:4040
[2024-01-05T16:50:20.696+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-01-05T16:50:20.807+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:20 INFO MemoryStore: MemoryStore cleared
[2024-01-05T16:50:20.807+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:20 INFO BlockManager: BlockManager stopped
[2024-01-05T16:50:20.843+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:20 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-01-05T16:50:20.859+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-01-05T16:50:20.900+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:20 INFO SparkContext: Successfully stopped SparkContext
[2024-01-05T16:50:20.901+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:20 INFO ShutdownHookManager: Shutdown hook called
[2024-01-05T16:50:20.903+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-ae680f8d-19c3-4ec1-907f-a8a9498b7203
[2024-01-05T16:50:20.950+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-ae680f8d-19c3-4ec1-907f-a8a9498b7203/pyspark-52fe5131-102c-43d2-8a70-78adfb0e1d3f
[2024-01-05T16:50:20.983+0000] {spark_submit.py:536} INFO - 24/01/05 16:50:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-a979eef9-c127-4834-b789-3ec2b029f29e
[2024-01-05T16:50:21.211+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_data, execution_date=20240102T000000, start_date=20240105T164940, end_date=20240105T165021
[2024-01-05T16:50:21.249+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-01-05T16:50:21.269+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-05T17:47:10.691+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-02T00:00:00+00:00 [queued]>
[2024-01-05T17:47:10.708+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-02T00:00:00+00:00 [queued]>
[2024-01-05T17:47:10.715+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 1
[2024-01-05T17:47:10.754+0000] {taskinstance.py:2192} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_data> on 2024-01-02 00:00:00+00:00
[2024-01-05T17:47:10.761+0000] {standard_task_runner.py:60} INFO - Started process 28549 to run task
[2024-01-05T17:47:10.764+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_data', 'scheduled__2024-01-02T00:00:00+00:00', '--job-id', '74', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpw50rst1l']
[2024-01-05T17:47:10.766+0000] {standard_task_runner.py:88} INFO - Job 74: Subtask transform_twitter_data
[2024-01-05T17:47:10.850+0000] {task_command.py:423} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-02T00:00:00+00:00 [running]> on host airflow.vm.local
[2024-01-05T17:47:10.974+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_data' AIRFLOW_CTX_EXECUTION_DATE='2024-01-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-02T00:00:00+00:00'
[2024-01-05T17:47:10.982+0000] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-05T17:47:10.984+0000] {spark_submit.py:366} INFO - Spark-Submit cmd: spark-submit --master local --name Twitter Transformation --queue root.default /home/eder/Desktop/Airflow/Twitter/src/spark/transformation.py --src /home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-02 --dest /home/eder/Desktop/Airflow/Twitter/data/processed/ --process_date 2024-01-02
[2024-01-05T17:47:13.271+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:13 WARN Utils: Your hostname, airflow resolves to a loopback address: 127.0.1.1; using 192.168.1.79 instead (on interface enp0s3)
[2024-01-05T17:47:13.273+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-01-05T17:47:14.142+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-05T17:47:14.868+0000] {spark_submit.py:536} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-01-05T17:47:14.889+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:14 INFO SparkContext: Running Spark version 3.1.3
[2024-01-05T17:47:14.971+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:14 INFO ResourceUtils: ==============================================================
[2024-01-05T17:47:14.971+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:14 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-05T17:47:14.972+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:14 INFO ResourceUtils: ==============================================================
[2024-01-05T17:47:14.973+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:14 INFO SparkContext: Submitted application: Data Transformation
[2024-01-05T17:47:15.015+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-05T17:47:15.030+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO ResourceProfile: Limiting resource is cpu
[2024-01-05T17:47:15.031+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-05T17:47:15.121+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO SecurityManager: Changing view acls to: eder
[2024-01-05T17:47:15.122+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO SecurityManager: Changing modify acls to: eder
[2024-01-05T17:47:15.124+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO SecurityManager: Changing view acls groups to:
[2024-01-05T17:47:15.124+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO SecurityManager: Changing modify acls groups to:
[2024-01-05T17:47:15.125+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(eder); groups with view permissions: Set(); users  with modify permissions: Set(eder); groups with modify permissions: Set()
[2024-01-05T17:47:15.570+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO Utils: Successfully started service 'sparkDriver' on port 39685.
[2024-01-05T17:47:15.653+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO SparkEnv: Registering MapOutputTracker
[2024-01-05T17:47:15.722+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-05T17:47:15.770+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-05T17:47:15.771+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-05T17:47:15.779+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-05T17:47:15.807+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d2116eea-6f41-430f-8029-6f62a82ea28a
[2024-01-05T17:47:15.862+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-01-05T17:47:15.949+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:15 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-05T17:47:16.561+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-05T17:47:16.747+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://airflow.Home:4040
[2024-01-05T17:47:17.238+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:17 INFO Executor: Starting executor ID driver on host airflow.Home
[2024-01-05T17:47:17.303+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41283.
[2024-01-05T17:47:17.304+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:17 INFO NettyBlockTransferService: Server created on airflow.Home:41283
[2024-01-05T17:47:17.306+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-05T17:47:17.323+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, airflow.Home, 41283, None)
[2024-01-05T17:47:17.330+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:17 INFO BlockManagerMasterEndpoint: Registering block manager airflow.Home:41283 with 366.3 MiB RAM, BlockManagerId(driver, airflow.Home, 41283, None)
[2024-01-05T17:47:17.338+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, airflow.Home, 41283, None)
[2024-01-05T17:47:17.340+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, airflow.Home, 41283, None)
[2024-01-05T17:47:18.508+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/eder/Desktop/Airflow/Twitter/spark-warehouse').
[2024-01-05T17:47:18.509+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:18 INFO SharedState: Warehouse path is 'file:/home/eder/Desktop/Airflow/Twitter/spark-warehouse'.
[2024-01-05T17:47:19.925+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:19 INFO InMemoryFileIndex: It took 115 ms to list leaf files for 1 paths.
[2024-01-05T17:47:20.083+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:20 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-01-05T17:47:23.559+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:23 INFO FileSourceStrategy: Pushed Filters:
[2024-01-05T17:47:23.562+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:23 INFO FileSourceStrategy: Post-Scan Filters:
[2024-01-05T17:47:23.569+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:23 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-01-05T17:47:24.103+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-01-05T17:47:24.210+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 366.0 MiB)
[2024-01-05T17:47:24.215+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on airflow.Home:41283 (size: 27.4 KiB, free: 366.3 MiB)
[2024-01-05T17:47:24.251+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T17:47:24.279+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4226248 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T17:47:24.568+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T17:47:24.615+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T17:47:24.615+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T17:47:24.616+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T17:47:24.618+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO DAGScheduler: Missing parents: List()
[2024-01-05T17:47:24.634+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T17:47:24.811+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 366.0 MiB)
[2024-01-05T17:47:24.815+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-01-05T17:47:24.820+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on airflow.Home:41283 (size: 6.3 KiB, free: 366.3 MiB)
[2024-01-05T17:47:24.825+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-01-05T17:47:24.854+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T17:47:24.885+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-01-05T17:47:24.992+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
[2024-01-05T17:47:25.028+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-01-05T17:47:25.548+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:25 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-02/%202024-01-02.json, range: 0-31944, partition values: [empty row]
[2024-01-05T17:47:26.207+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:26 INFO CodeGenerator: Code generated in 475.226111 ms
[2024-01-05T17:47:26.322+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-01-05T17:47:26.341+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1364 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T17:47:26.506+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-01-05T17:47:26.518+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:26 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.850 s
[2024-01-05T17:47:26.527+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T17:47:26.530+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-01-05T17:47:26.547+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:26 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.973595 s
[2024-01-05T17:47:27.225+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-01-05T17:47:27.231+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-01-05T17:47:27.231+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-01-05T17:47:27.364+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T17:47:27.371+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T17:47:27.372+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T17:47:27.594+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO CodeGenerator: Code generated in 55.972042 ms
[2024-01-05T17:47:27.685+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO CodeGenerator: Code generated in 55.141483 ms
[2024-01-05T17:47:27.693+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-01-05T17:47:27.702+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-01-05T17:47:27.711+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on airflow.Home:41283 (size: 27.5 KiB, free: 366.2 MiB)
[2024-01-05T17:47:27.732+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T17:47:27.735+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4226248 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T17:47:27.827+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T17:47:27.832+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T17:47:27.837+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T17:47:27.838+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T17:47:27.838+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO DAGScheduler: Missing parents: List()
[2024-01-05T17:47:27.840+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:27 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T17:47:28.254+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 189.7 KiB, free 365.5 MiB)
[2024-01-05T17:47:28.256+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.4 KiB, free 365.4 MiB)
[2024-01-05T17:47:28.259+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on airflow.Home:41283 (size: 66.4 KiB, free: 366.2 MiB)
[2024-01-05T17:47:28.259+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-01-05T17:47:28.260+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T17:47:28.260+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-01-05T17:47:28.268+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 5134 bytes) taskResourceAssignments Map()
[2024-01-05T17:47:28.270+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-01-05T17:47:28.372+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T17:47:28.375+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T17:47:28.380+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T17:47:28.531+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO CodeGenerator: Code generated in 35.310328 ms
[2024-01-05T17:47:28.546+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-02/%202024-01-02.json, range: 0-31944, partition values: [empty row]
[2024-01-05T17:47:28.583+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO CodeGenerator: Code generated in 33.108356 ms
[2024-01-05T17:47:28.626+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO CodeGenerator: Code generated in 10.359771 ms
[2024-01-05T17:47:28.718+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_20240105174727691663318087253647_0001_m_000000_1' to file:/home/eder/Desktop/Airflow/Twitter/data/processed/tweets/2024-01-02/_temporary/0/task_20240105174727691663318087253647_0001_m_000000
[2024-01-05T17:47:28.720+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO SparkHadoopMapRedUtil: attempt_20240105174727691663318087253647_0001_m_000000_1: Committed
[2024-01-05T17:47:28.728+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-01-05T17:47:28.730+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 469 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T17:47:28.731+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-01-05T17:47:28.732+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.890 s
[2024-01-05T17:47:28.735+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T17:47:28.735+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-01-05T17:47:28.738+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.910422 s
[2024-01-05T17:47:28.794+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO FileFormatWriter: Write Job 07c6e0c0-6cbb-49bc-961e-60e4fb6b017c committed.
[2024-01-05T17:47:28.812+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO FileFormatWriter: Finished processing stats for write job 07c6e0c0-6cbb-49bc-961e-60e4fb6b017c.
[2024-01-05T17:47:28.888+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO FileSourceStrategy: Pushed Filters:
[2024-01-05T17:47:28.890+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO FileSourceStrategy: Post-Scan Filters:
[2024-01-05T17:47:28.891+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-01-05T17:47:28.913+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T17:47:28.913+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T17:47:28.913+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T17:47:29.040+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO CodeGenerator: Code generated in 30.239535 ms
[2024-01-05T17:47:29.059+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-01-05T17:47:29.087+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-01-05T17:47:29.088+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on airflow.Home:41283 (size: 27.5 KiB, free: 366.1 MiB)
[2024-01-05T17:47:29.102+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T17:47:29.104+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4226248 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T17:47:29.133+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T17:47:29.135+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T17:47:29.135+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T17:47:29.135+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T17:47:29.135+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO DAGScheduler: Missing parents: List()
[2024-01-05T17:47:29.182+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T17:47:29.265+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.0 KiB, free 364.9 MiB)
[2024-01-05T17:47:29.267+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 63.8 KiB, free 364.8 MiB)
[2024-01-05T17:47:29.268+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on airflow.Home:41283 (size: 63.8 KiB, free: 366.1 MiB)
[2024-01-05T17:47:29.270+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-01-05T17:47:29.271+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T17:47:29.272+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-01-05T17:47:29.274+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 5134 bytes) taskResourceAssignments Map()
[2024-01-05T17:47:29.275+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-01-05T17:47:29.288+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T17:47:29.289+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T17:47:29.289+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T17:47:29.414+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO CodeGenerator: Code generated in 28.63094 ms
[2024-01-05T17:47:29.419+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-02/%202024-01-02.json, range: 0-31944, partition values: [empty row]
[2024-01-05T17:47:29.443+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO CodeGenerator: Code generated in 20.913423 ms
[2024-01-05T17:47:29.463+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO FileOutputCommitter: Saved output of task 'attempt_202401051747292790436505696815144_0002_m_000000_2' to file:/home/eder/Desktop/Airflow/Twitter/data/processed/users/2024-01-02/_temporary/0/task_202401051747292790436505696815144_0002_m_000000
[2024-01-05T17:47:29.464+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO SparkHadoopMapRedUtil: attempt_202401051747292790436505696815144_0002_m_000000_2: Committed
[2024-01-05T17:47:29.465+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-01-05T17:47:29.468+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 193 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T17:47:29.470+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-01-05T17:47:29.471+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.280 s
[2024-01-05T17:47:29.473+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T17:47:29.474+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-01-05T17:47:29.474+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.340712 s
[2024-01-05T17:47:29.495+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO FileFormatWriter: Write Job 8372c12e-f51f-4ca4-9229-1025b1cbc973 committed.
[2024-01-05T17:47:29.495+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO FileFormatWriter: Finished processing stats for write job 8372c12e-f51f-4ca4-9229-1025b1cbc973.
[2024-01-05T17:47:29.563+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO SparkContext: Invoking stop() from shutdown hook
[2024-01-05T17:47:29.634+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO SparkUI: Stopped Spark web UI at http://airflow.Home:4040
[2024-01-05T17:47:29.695+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-01-05T17:47:29.732+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO MemoryStore: MemoryStore cleared
[2024-01-05T17:47:29.743+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO BlockManager: BlockManager stopped
[2024-01-05T17:47:29.770+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-01-05T17:47:29.781+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-01-05T17:47:29.812+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO SparkContext: Successfully stopped SparkContext
[2024-01-05T17:47:29.814+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO ShutdownHookManager: Shutdown hook called
[2024-01-05T17:47:29.819+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-c2cd3871-778b-4a6c-b0c8-db20150524f7
[2024-01-05T17:47:29.834+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-34a551d7-fb00-4463-89c2-a164f820c7d1
[2024-01-05T17:47:29.842+0000] {spark_submit.py:536} INFO - 24/01/05 17:47:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-c2cd3871-778b-4a6c-b0c8-db20150524f7/pyspark-afe802d6-ad65-4cf5-8d5e-b2f5796e5429
[2024-01-05T17:47:29.946+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_data, execution_date=20240102T000000, start_date=20240105T174710, end_date=20240105T174729
[2024-01-05T17:47:29.988+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-01-05T17:47:29.999+0000] {taskinstance.py:3281} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-01-05T18:05:44.332+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-02T00:00:00+00:00 [queued]>
[2024-01-05T18:05:44.360+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-02T00:00:00+00:00 [queued]>
[2024-01-05T18:05:44.363+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 1
[2024-01-05T18:05:44.515+0000] {taskinstance.py:2192} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_data> on 2024-01-02 00:00:00+00:00
[2024-01-05T18:05:44.520+0000] {standard_task_runner.py:60} INFO - Started process 39124 to run task
[2024-01-05T18:05:44.525+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_data', 'scheduled__2024-01-02T00:00:00+00:00', '--job-id', '97', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpaxn2leuv']
[2024-01-05T18:05:44.525+0000] {standard_task_runner.py:88} INFO - Job 97: Subtask transform_twitter_data
[2024-01-05T18:05:44.602+0000] {task_command.py:423} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_data scheduled__2024-01-02T00:00:00+00:00 [running]> on host airflow.vm.local
[2024-01-05T18:05:44.858+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='TwitterDAG' AIRFLOW_CTX_TASK_ID='transform_twitter_data' AIRFLOW_CTX_EXECUTION_DATE='2024-01-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-02T00:00:00+00:00'
[2024-01-05T18:05:44.867+0000] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-05T18:05:44.868+0000] {spark_submit.py:366} INFO - Spark-Submit cmd: spark-submit --master local --name Twitter Transformation --queue root.default /home/eder/Desktop/Airflow/Twitter/src/spark/transformation.py --src /home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-02 --dest /home/eder/Desktop/Airflow/Twitter/data/processed/ --process_date 2024-01-02
[2024-01-05T18:05:48.192+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:48 WARN Utils: Your hostname, airflow resolves to a loopback address: 127.0.1.1; using 192.168.1.79 instead (on interface enp0s3)
[2024-01-05T18:05:48.201+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-01-05T18:05:49.476+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-05T18:05:50.413+0000] {spark_submit.py:536} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-01-05T18:05:50.436+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:50 INFO SparkContext: Running Spark version 3.1.3
[2024-01-05T18:05:50.522+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:50 INFO ResourceUtils: ==============================================================
[2024-01-05T18:05:50.522+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:50 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-05T18:05:50.523+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:50 INFO ResourceUtils: ==============================================================
[2024-01-05T18:05:50.528+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:50 INFO SparkContext: Submitted application: Data Transformation
[2024-01-05T18:05:50.587+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-05T18:05:50.606+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:50 INFO ResourceProfile: Limiting resource is cpu
[2024-01-05T18:05:50.606+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:50 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-05T18:05:50.712+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:50 INFO SecurityManager: Changing view acls to: eder
[2024-01-05T18:05:50.713+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:50 INFO SecurityManager: Changing modify acls to: eder
[2024-01-05T18:05:50.713+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:50 INFO SecurityManager: Changing view acls groups to:
[2024-01-05T18:05:50.713+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:50 INFO SecurityManager: Changing modify acls groups to:
[2024-01-05T18:05:50.713+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(eder); groups with view permissions: Set(); users  with modify permissions: Set(eder); groups with modify permissions: Set()
[2024-01-05T18:05:51.342+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:51 INFO Utils: Successfully started service 'sparkDriver' on port 33257.
[2024-01-05T18:05:51.414+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:51 INFO SparkEnv: Registering MapOutputTracker
[2024-01-05T18:05:51.547+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:51 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-05T18:05:51.618+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-05T18:05:51.619+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-05T18:05:51.627+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-05T18:05:51.657+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-152eea75-57d0-4c8b-9931-3ad6ca1a00ed
[2024-01-05T18:05:51.703+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:51 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-01-05T18:05:51.890+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:51 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-05T18:05:52.429+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-05T18:05:52.680+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://airflow.Home:4040
[2024-01-05T18:05:53.624+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:53 INFO Executor: Starting executor ID driver on host airflow.Home
[2024-01-05T18:05:53.711+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36485.
[2024-01-05T18:05:53.711+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:53 INFO NettyBlockTransferService: Server created on airflow.Home:36485
[2024-01-05T18:05:53.716+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-05T18:05:53.744+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, airflow.Home, 36485, None)
[2024-01-05T18:05:53.769+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:53 INFO BlockManagerMasterEndpoint: Registering block manager airflow.Home:36485 with 366.3 MiB RAM, BlockManagerId(driver, airflow.Home, 36485, None)
[2024-01-05T18:05:53.783+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, airflow.Home, 36485, None)
[2024-01-05T18:05:53.787+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, airflow.Home, 36485, None)
[2024-01-05T18:05:55.753+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/eder/Desktop/Airflow/Twitter/spark-warehouse').
[2024-01-05T18:05:55.754+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:55 INFO SharedState: Warehouse path is 'file:/home/eder/Desktop/Airflow/Twitter/spark-warehouse'.
[2024-01-05T18:05:57.783+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:57 INFO InMemoryFileIndex: It took 329 ms to list leaf files for 1 paths.
[2024-01-05T18:05:57.970+0000] {spark_submit.py:536} INFO - 24/01/05 18:05:57 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
[2024-01-05T18:06:02.578+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:02 INFO FileSourceStrategy: Pushed Filters:
[2024-01-05T18:06:02.579+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:02 INFO FileSourceStrategy: Post-Scan Filters:
[2024-01-05T18:06:02.586+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-01-05T18:06:03.343+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-01-05T18:06:03.522+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 366.0 MiB)
[2024-01-05T18:06:03.535+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on airflow.Home:36485 (size: 27.4 KiB, free: 366.3 MiB)
[2024-01-05T18:06:03.567+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:03 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T18:06:03.594+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207917 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T18:06:03.953+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:03 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T18:06:03.989+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:03 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T18:06:03.990+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:03 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T18:06:03.990+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:03 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T18:06:03.994+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:03 INFO DAGScheduler: Missing parents: List()
[2024-01-05T18:06:04.016+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T18:06:04.229+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 366.0 MiB)
[2024-01-05T18:06:04.254+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-01-05T18:06:04.255+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on airflow.Home:36485 (size: 6.3 KiB, free: 366.3 MiB)
[2024-01-05T18:06:04.265+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-01-05T18:06:04.288+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T18:06:04.290+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-01-05T18:06:04.472+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
[2024-01-05T18:06:04.567+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-01-05T18:06:05.338+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:05 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-02/%202024-01-02.json, range: 0-13613, partition values: [empty row]
[2024-01-05T18:06:05.993+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:05 INFO CodeGenerator: Code generated in 457.977297 ms
[2024-01-05T18:06:06.105+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-01-05T18:06:06.122+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1703 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T18:06:06.127+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-01-05T18:06:06.314+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:06 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 2.239 s
[2024-01-05T18:06:06.323+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T18:06:06.324+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-01-05T18:06:06.330+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:06 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 2.375201 s
[2024-01-05T18:06:07.591+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:07 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-01-05T18:06:07.594+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:07 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-01-05T18:06:07.600+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:07 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-01-05T18:06:07.755+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T18:06:07.756+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T18:06:07.757+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T18:06:08.066+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO CodeGenerator: Code generated in 113.334118 ms
[2024-01-05T18:06:08.150+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO CodeGenerator: Code generated in 42.001032 ms
[2024-01-05T18:06:08.162+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-01-05T18:06:08.171+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-01-05T18:06:08.173+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on airflow.Home:36485 (size: 27.5 KiB, free: 366.2 MiB)
[2024-01-05T18:06:08.174+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T18:06:08.178+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207917 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T18:06:08.332+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T18:06:08.333+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T18:06:08.333+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T18:06:08.333+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T18:06:08.333+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO DAGScheduler: Missing parents: List()
[2024-01-05T18:06:08.334+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T18:06:08.488+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 189.7 KiB, free 365.5 MiB)
[2024-01-05T18:06:08.491+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.3 KiB, free 365.4 MiB)
[2024-01-05T18:06:08.492+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on airflow.Home:36485 (size: 66.3 KiB, free: 366.2 MiB)
[2024-01-05T18:06:08.498+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-01-05T18:06:08.498+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T18:06:08.498+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-01-05T18:06:08.509+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 5134 bytes) taskResourceAssignments Map()
[2024-01-05T18:06:08.521+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-01-05T18:06:08.694+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T18:06:08.694+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T18:06:08.697+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T18:06:09.042+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO CodeGenerator: Code generated in 50.654679 ms
[2024-01-05T18:06:09.053+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-02/%202024-01-02.json, range: 0-13613, partition values: [empty row]
[2024-01-05T18:06:09.097+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO CodeGenerator: Code generated in 41.436712 ms
[2024-01-05T18:06:09.146+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO CodeGenerator: Code generated in 12.260265 ms
[2024-01-05T18:06:09.274+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO FileOutputCommitter: Saved output of task 'attempt_202401051806081831697731802183278_0001_m_000000_1' to file:/home/eder/Desktop/Airflow/Twitter/data/processed/tweets/2024-01-02/_temporary/0/task_202401051806081831697731802183278_0001_m_000000
[2024-01-05T18:06:09.283+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO SparkHadoopMapRedUtil: attempt_202401051806081831697731802183278_0001_m_000000_1: Committed
[2024-01-05T18:06:09.296+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-01-05T18:06:09.322+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 821 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T18:06:09.327+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-01-05T18:06:09.333+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.998 s
[2024-01-05T18:06:09.333+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T18:06:09.333+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-01-05T18:06:09.352+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 1.011039 s
[2024-01-05T18:06:09.428+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO FileFormatWriter: Write Job 12aa3713-4307-46ec-b641-0d8e6cb9ade0 committed.
[2024-01-05T18:06:09.439+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO FileFormatWriter: Finished processing stats for write job 12aa3713-4307-46ec-b641-0d8e6cb9ade0.
[2024-01-05T18:06:09.552+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO FileSourceStrategy: Pushed Filters:
[2024-01-05T18:06:09.554+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO FileSourceStrategy: Post-Scan Filters:
[2024-01-05T18:06:09.554+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-01-05T18:06:09.567+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T18:06:09.571+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T18:06:09.574+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T18:06:09.659+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO CodeGenerator: Code generated in 18.367769 ms
[2024-01-05T18:06:09.662+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-01-05T18:06:09.671+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-01-05T18:06:09.672+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on airflow.Home:36485 (size: 27.5 KiB, free: 366.1 MiB)
[2024-01-05T18:06:09.674+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-01-05T18:06:09.683+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207917 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-05T18:06:09.701+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-01-05T18:06:09.702+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-01-05T18:06:09.702+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-01-05T18:06:09.703+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO DAGScheduler: Parents of final stage: List()
[2024-01-05T18:06:09.703+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO DAGScheduler: Missing parents: List()
[2024-01-05T18:06:09.722+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-01-05T18:06:09.826+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.0 KiB, free 364.9 MiB)
[2024-01-05T18:06:09.829+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 63.8 KiB, free 364.8 MiB)
[2024-01-05T18:06:09.834+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on airflow.Home:36485 (size: 63.8 KiB, free: 366.1 MiB)
[2024-01-05T18:06:09.836+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-01-05T18:06:09.838+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-01-05T18:06:09.839+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-01-05T18:06:09.840+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (airflow.Home, executor driver, partition 0, PROCESS_LOCAL, 5134 bytes) taskResourceAssignments Map()
[2024-01-05T18:06:09.842+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-01-05T18:06:09.865+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-05T18:06:09.865+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-05T18:06:09.865+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-01-05T18:06:10.165+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO CodeGenerator: Code generated in 19.996195 ms
[2024-01-05T18:06:10.172+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO FileScanRDD: Reading File path: file:///home/eder/Desktop/Airflow/Twitter/data/raw/2024-01-02/%202024-01-02.json, range: 0-13613, partition values: [empty row]
[2024-01-05T18:06:10.210+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO CodeGenerator: Code generated in 35.274453 ms
[2024-01-05T18:06:10.227+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO FileOutputCommitter: Saved output of task 'attempt_202401051806096597756457425798215_0002_m_000000_2' to file:/home/eder/Desktop/Airflow/Twitter/data/processed/users/2024-01-02/_temporary/0/task_202401051806096597756457425798215_0002_m_000000
[2024-01-05T18:06:10.228+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO SparkHadoopMapRedUtil: attempt_202401051806096597756457425798215_0002_m_000000_2: Committed
[2024-01-05T18:06:10.233+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-01-05T18:06:10.237+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 397 ms on airflow.Home (executor driver) (1/1)
[2024-01-05T18:06:10.238+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-01-05T18:06:10.239+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.516 s
[2024-01-05T18:06:10.239+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-05T18:06:10.240+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-01-05T18:06:10.240+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.538560 s
[2024-01-05T18:06:10.291+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO FileFormatWriter: Write Job 274afc18-ba1f-4aa0-be7f-4ffb33f57a11 committed.
[2024-01-05T18:06:10.291+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO FileFormatWriter: Finished processing stats for write job 274afc18-ba1f-4aa0-be7f-4ffb33f57a11.
[2024-01-05T18:06:10.722+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO SparkContext: Invoking stop() from shutdown hook
[2024-01-05T18:06:10.819+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO BlockManagerInfo: Removed broadcast_3_piece0 on airflow.Home:36485 in memory (size: 66.3 KiB, free: 366.2 MiB)
[2024-01-05T18:06:10.841+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:10 INFO SparkUI: Stopped Spark web UI at http://airflow.Home:4040
[2024-01-05T18:06:11.011+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-01-05T18:06:11.079+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:11 INFO MemoryStore: MemoryStore cleared
[2024-01-05T18:06:11.088+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:11 INFO BlockManager: BlockManager stopped
[2024-01-05T18:06:11.106+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:11 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-01-05T18:06:11.113+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-01-05T18:06:11.142+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:11 INFO SparkContext: Successfully stopped SparkContext
[2024-01-05T18:06:11.142+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:11 INFO ShutdownHookManager: Shutdown hook called
[2024-01-05T18:06:11.143+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-ebdf3439-9bf5-462d-becd-437193424ba9/pyspark-ed0a9917-a189-4c57-b6e4-c50c3c560547
[2024-01-05T18:06:11.171+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-22e709f3-b13e-41fd-9781-a9954feabdf4
[2024-01-05T18:06:11.197+0000] {spark_submit.py:536} INFO - 24/01/05 18:06:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-ebdf3439-9bf5-462d-becd-437193424ba9
[2024-01-05T18:06:11.334+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_data, execution_date=20240102T000000, start_date=20240105T180544, end_date=20240105T180611
[2024-01-05T18:06:11.461+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-01-05T18:06:11.478+0000] {taskinstance.py:3281} INFO - 1 downstream tasks scheduled from follow-on schedule check
